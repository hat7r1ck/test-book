# Authentication Analysis Notebook

import pandas as pd
import numpy as np
import seaborn as sns
import scipy.stats as stats
import matplotlib.pyplot as plt
import ipywidgets as widgets
from IPython.display import display, clear_output
import warnings
warnings.filterwarnings('ignore')

# Configuration settings
config = {
    'weights': {
        'source_ip': 10,
        'device_id': 15,
        'user_agent': 10,
        'login_hour': 10,
        'authentication_type': 10,
        'authentication_result': 10,
        'location': 15,
        'application': 10,
        'carrier_name': 10
    },
    'thresholds': {
        'low_risk': [0, 0],
        'moderate_risk': [1, 50],
        'high_risk': [51, 85],
        'critical_risk': [86, 100]
    },
    'risk_level_recommendations': {
        'Low Risk': "No immediate action required.",
        'Moderate Risk': "Review the event for any anomalies.",
        'High Risk': "Investigate the event for potential security issues.",
        'Critical Risk': "Immediate investigation is required. Escalate to the security team."
    }
}

# Function to load the data with error handling
def load_data(csv_file_path):
    try:
        df = pd.read_csv(csv_file_path)
        print("Data loaded successfully.")
        return df
    except FileNotFoundError:
        print("Error: The file was not found. Please check the file path.")
        return None
    except pd.errors.ParserError:
        print("Error: The file could not be parsed. Please ensure it is a valid CSV file.")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

# Widgets for dataset input
csv_file_input = widgets.Text(
    value='',
    placeholder='Enter CSV file path',
    description='CSV File:',
    disabled=False
)

dataset_box = widgets.VBox([
    widgets.Label(value="**Dataset Input:**"),
    csv_file_input
])

# Widgets for alert event input
alert_fields = [
    ('timestamps', 'Timestamp:', 'Enter Timestamp'),
    ('udm.principal.user.userid', 'User ID:', 'Enter User ID'),
    ('udm.target.user.userid', 'Alternate User ID:', 'Enter User ID (if applicable)'),
    ('udm.principal.ip', 'Source IP:', 'Enter Source IP'),
    ('udm.principal.asset_id', 'Device ID:', 'Enter Device ID'),
    ('udm.network.http.user_agent', 'User Agent:', 'Enter User Agent'),
    ('udm.metadata.product_name', 'Authentication Type:', 'Enter Authentication Type'),
    ('udm.security_result.action', 'Authentication Result:', 'Enter Authentication Result'),
    ('udm.principal.ip_geo_artifact.location.country_or_region', 'Country:', 'Enter Country'),
    ('udm.principal.ip_geo_artifact.location.state', 'State:', 'Enter State'),
    ('udm.target.application', 'Application:', 'Enter Application'),
    ('udm.principal.ip_geo_artifact.network.carrier_name', 'Carrier Name:', 'Enter Carrier Name')
]

alert_widgets = []
for field_name, description, placeholder in alert_fields:
    widget = widgets.Text(
        value='',
        placeholder=placeholder,
        description=description,
        disabled=False
    )
    alert_widgets.append((field_name, widget))

alert_box = widgets.VBox([
    widgets.Label(value="**Alert Event Details:**")
] + [w for _, w in alert_widgets])

# Output widget for messages
message_output = widgets.Output()

# Submit button
submit_button = widgets.Button(
    description='Analyze',
    disabled=False,
    button_style='success',
    tooltip='Click to analyze the alert event',
    icon='check'
)

# Display the widgets
display(dataset_box, alert_box, submit_button, message_output)

def on_submit_button_clicked(b):
    with message_output:
        clear_output()
        # Load data
        csv_file_path = csv_file_input.value.strip()
        df = load_data(csv_file_path)
        if df is None:
            return

        # Collect alert event details
        alert_event = {}
        for field_name, widget in alert_widgets:
            alert_event[field_name] = widget.value.strip()
        
        # Handle alternate 'user_id' fields
        user_id_primary = alert_event.get('udm.principal.user.userid', '').lower()
        user_id_alternate = alert_event.get('udm.target.user.userid', '').lower()
        
        if not user_id_primary and not user_id_alternate:
            print("Error: User ID is missing. Please provide 'User ID' or 'Alternate User ID'.")
            return
        elif user_id_primary:
            alert_event['user_id'] = user_id_primary
        else:
            alert_event['user_id'] = user_id_alternate

        # Input validation
        required_fields = ['timestamps', 'udm.principal.ip', 'udm.principal.asset_id',
                           'udm.network.http.user_agent', 'udm.metadata.product_name', 'udm.security_result.action',
                           'udm.principal.ip_geo_artifact.location.country_or_region', 'udm.principal.ip_geo_artifact.location.state']
        
        missing_fields = [field_name for field_name in required_fields if not alert_event.get(field_name)]
        if missing_fields:
            missing_descriptions = [description for field_name_loop, description, _ in alert_fields if field_name_loop in missing_fields]
            print(f"Error: The following fields are missing or empty: {', '.join(missing_descriptions)}")
            return

        # Proceed with analysis
        perform_analysis(df, alert_event)

def perform_analysis(df, alert_event):
    # Map SIEM columns to analysis fields
    column_mapping = {
        'udm.principal.ip': 'source_ip',
        'udm.network.http.user_agent': 'user_agent',
        'udm.metadata.product_name': 'authentication_type',
        'udm.security_result.action': 'authentication_result',
        'udm.principal.user.userid': 'user_id',
        'udm.target.user.userid': 'user_id',  # Alternate user_id field
        'udm.principal.asset_id': 'device_id',
        'udm.principal.ip_geo_artifact.location.country_or_region': 'country',
        'udm.principal.ip_geo_artifact.location.state': 'state',
        'udm.target.application': 'application',
        'udm.principal.ip_geo_artifact.network.carrier_name': 'carrier_name',
        'timestamps': 'timestamp'
    }

    # Check for both possible 'user_id' columns in the dataset
    if 'udm.principal.user.userid' in df.columns:
        df['user_id'] = df['udm.principal.user.userid']
    elif 'udm.target.user.userid' in df.columns:
        df['user_id'] = df['udm.target.user.userid']
    else:
        print("Error: Neither 'udm.principal.user.userid' nor 'udm.target.user.userid' found in dataset.")
        return

    # Rename columns based on mapping, excluding 'user_id' as it's already handled
    columns_to_rename = {k: v for k, v in column_mapping.items() if k in df.columns and v != 'user_id'}
    df.rename(columns=columns_to_rename, inplace=True)

    # Ensure the 'timestamp' column is in datetime format
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df.dropna(subset=['timestamp'], inplace=True)

    # Normalize fields
    fields_to_normalize = ['user_id', 'device_id', 'authentication_type', 'user_agent',
                           'country', 'state', 'authentication_result', 'application', 'carrier_name', 'source_ip']

    for field in fields_to_normalize:
        if field in df.columns:
            df[field] = df[field].astype(str).str.strip().str.lower()
        else:
            df[field] = 'unknown'

    # Normalize alert event fields
    # Since 'user_id' is already handled, ensure other fields are normalized
    alert_event_mapped = {}
    for original_field, internal_field in column_mapping.items():
        if internal_field in fields_to_normalize:
            if original_field == 'udm.principal.user.userid' or original_field == 'udm.target.user.userid':
                # 'user_id' is already handled
                continue
            value = alert_event.get(original_field, 'unknown').strip().lower()
            alert_event_mapped[internal_field] = value

    # Handle 'timestamp' separately
    try:
        alert_timestamp = pd.to_datetime(alert_event['timestamps'], errors='coerce')
        if pd.isnull(alert_timestamp):
            raise ValueError
        alert_event_mapped['timestamp'] = alert_timestamp
    except ValueError:
        print("Error: Invalid timestamp format. Please ensure it matches the CSV's timestamp format.")
        return

    # Add 'user_id' to mapped alert_event
    alert_event_mapped['user_id'] = alert_event['user_id']

    # Filter user history
    alert_user_id = alert_event_mapped['user_id']
    user_history = df[df['user_id'] == alert_user_id]
    total_events = len(user_history)

    if total_events == 0:
        print(f"No historical data found for user '{alert_user_id}'.")
        return

    # Initialize findings
    detailed_findings = {}
    risk_score_components = {}

    # Define characteristics and weights from config
    characteristics = {
        'Source IP Address': ('source_ip', config['weights']['source_ip']),
        'Device ID': ('device_id', config['weights']['device_id']),
        'User Agent': ('user_agent', config['weights']['user_agent']),
        'Login Hour': ('hour', config['weights']['login_hour']),
        'Authentication Type': ('authentication_type', config['weights']['authentication_type']),
        'Authentication Result': ('authentication_result', config['weights']['authentication_result']),
        'Location': ('location', config['weights']['location']),
        'Application': ('application', config['weights']['application']),
        'Carrier Name': ('carrier_name', config['weights']['carrier_name'])
    }

    # Perform analysis
    for characteristic, (field_name, weight) in characteristics.items():
        if characteristic == 'Login Hour':
            alert_value = alert_event_mapped['timestamp'].hour
            user_history['hour'] = user_history['timestamp'].dt.hour
            field_series = user_history['hour']
        elif characteristic == 'Location':
            location_matches = user_history[
                (user_history['country'] == alert_event_mapped['country']) &
                (user_history['state'] == alert_event_mapped['state'])
            ]
            usage_count = len(location_matches)
            usage_percentage = (usage_count / total_events) * 100 if total_events > 0 else 0
            used_before = usage_count > 0
            detailed_findings[characteristic] = {
                'used_before': used_before,
                'usage_count': usage_count,
                'usage_percentage': usage_percentage,
                'weight': weight
            }
            risk_score_components[characteristic] = 0 if used_before else weight
            continue
        else:
            alert_value = alert_event_mapped[field_name]
            if field_name in user_history.columns:
                field_series = user_history[field_name]
            else:
                field_series = pd.Series()

        used_before, usage_count, usage_percentage = calculate_usage_count_and_percentage(field_series, alert_value)
        detailed_findings[characteristic] = {
            'used_before': used_before,
            'usage_count': usage_count,
            'usage_percentage': usage_percentage,
            'weight': weight
        }
        risk_score_components[characteristic] = 0 if used_before else weight

    # Calculate total risk score
    total_risk_score = sum(risk_score_components.values())
    max_possible_score = sum(weight for _, weight in characteristics.values())
    risk_percentage = (total_risk_score / max_possible_score) * 100

    # Determine risk level
    thresholds = config['thresholds']
    if risk_percentage == 0:
        risk_level = 'Low Risk'
    elif risk_percentage <= thresholds['moderate_risk'][1]:
        risk_level = 'Moderate Risk'
    elif risk_percentage <= thresholds['high_risk'][1]:
        risk_level = 'High Risk'
    else:
        risk_level = 'Critical Risk'

    conclusion = config['risk_level_recommendations'][risk_level]

    # Display findings
    display_analysis_results(alert_event_mapped, detailed_findings, total_events, risk_percentage, risk_level, conclusion)

    # Perform anomaly detection
    perform_anomaly_detection(user_history, alert_event_mapped)

    # Visualizations
    visualize_data(user_history, alert_event_mapped)

    # Export to Excel
    export_to_excel(detailed_findings, risk_percentage, risk_level, conclusion)

def calculate_usage_count_and_percentage(user_history_field, alert_value):
    counts = user_history_field.value_counts()
    usage_count = counts.get(alert_value, 0)
    usage_percentage = (usage_count / len(user_history_field)) * 100 if len(user_history_field) > 0 else 0
    used_before = usage_count > 0
    return used_before, usage_count, usage_percentage

def display_analysis_results(alert_event, detailed_findings, total_events, risk_percentage, risk_level, conclusion):
    thresholds = config['thresholds']
    print("\n**Detailed Analysis of Alert Event Compared to User's Historical Data:**\n")
    print(f"**Total number of historical logins for user '{alert_event['user_id']}': {total_events}**\n")
    print("**Characteristic Analysis:**")
    for characteristic, data in detailed_findings.items():
        status = 'Yes' if data['used_before'] else 'No'
        usage_info = f"Used {int(data['usage_count'])} times ({data['usage_percentage']:.2f}% of logins)"
        weight_info = f"Weight: {data['weight']}%"
        print(f"- **{characteristic}** ({weight_info})\n  - **Seen Before:** {status}\n  - **{usage_info}**\n")
    
    # Output the risk score and thresholds
    print(f"**Total Risk Score:** {risk_percentage:.2f}% (on a scale from 0 to 100%)")
    print(f"**Risk Level:** {risk_level}\n")
    print("**Risk Thresholds:**")
    for level, (lower, upper) in thresholds.items():
        print(f"- **{level}:** {lower}% - {upper}% Risk")
    
    # Output the conclusion
    print(f"\n**Conclusion:** {conclusion}")
    print(f"**Recommendation:** {config['risk_level_recommendations'][risk_level]}")

def perform_anomaly_detection(user_history, alert_event):
    print("\n**Anomaly Detection:**")
    if len(user_history) > 1:
        # Anomaly detection on login hours
        login_hours = user_history['timestamp'].dt.hour
        if login_hours.std() == 0:
            print("- Cannot perform anomaly detection on login hours due to zero standard deviation.")
        else:
            z_scores = np.abs(stats.zscore(login_hours))
            threshold = 2
            anomalies = login_hours[z_scores > threshold]
            
            alert_hour = alert_event['timestamp'].hour
            alert_hour_z_score = np.abs((alert_hour - login_hours.mean()) / login_hours.std())
            
            if alert_hour_z_score > threshold:
                print(f"- The login hour **{alert_hour}** is anomalous compared to the user's historical login hours.")
            else:
                print(f"- The login hour **{alert_hour}** is within the normal range.")
    else:
        print("- Not enough data for anomaly detection.")

def visualize_data(user_history, alert_event):
    # Visualize login hours
    if len(user_history) > 0:
        plt.figure(figsize=(10, 6))
        sns.histplot(user_history['timestamp'].dt.hour, bins=24, kde=False, color='skyblue', edgecolor='black')
        plt.axvline(alert_event['timestamp'].hour, color='red', linestyle='--', label='Alert Event Hour')
        plt.title(f"Login Hour Distribution for User '{alert_event['user_id']}'")
        plt.xlabel('Hour of Day')
        plt.ylabel('Number of Logins')
        plt.legend()
        plt.show()
    else:
        print("No historical data available for visualization.")

def export_to_excel(detailed_findings, risk_percentage, risk_level, conclusion):
    # Prepare data for export
    df_export = pd.DataFrame.from_dict(detailed_findings, orient='index')
    df_export.reset_index(inplace=True)
    df_export.rename(columns={'index': 'Characteristic'}, inplace=True)
    df_export['Risk Score (%)'] = risk_percentage
    df_export['Risk Level'] = risk_level
    df_export['Conclusion'] = conclusion

    # Export to Excel
    output_file = 'authentication_analysis_results.xlsx'
    try:
        df_export.to_excel(output_file, index=False)
        print(f"\n**Results have been exported to '{output_file}'.**")
    except Exception as e:
        print(f"\nError exporting results to Excel: {e}")
